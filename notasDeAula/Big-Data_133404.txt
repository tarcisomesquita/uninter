A01-P01
sumário

A01-P02
grande quantidade de dados
  documento de analógico para digital
  novos dispositivos (celular, internet das coisas)

Lei de Moore
  A capacidade de processamento dobra a cada 18 meses

A01-P03
Volume (data size), Velocity (speed of change), Variety (different forms of data source), Veracity (uncertainty of data)
Valor, Validade, Vulnerabilidade, Volatilidade, Visualização

Big data coleta dados que pode extrair informações

A01-P04
fontes: dados internos, datificação, dados de sensores, dados de fontes externas (internet)
armazenamento: escabilidade, disponibilidade, flexibilidade (suportar diferentes formatos)

NoSQL
chave-valor: chaves são indexadas, valor não.

A01-P05
escabilidade vertical (melhorar hardware) e horizontal (aumentar a quantidade de máquinas, que podem estar em diferentes lugares)

Hadoop
baixo custo, escabilidade, tolerância a falhas (partição) (pode ser de rede), balanceamento de carga, comunicação entre máquinas, alocação de máquinas

HDFS (Hadoop distributed file system)
MapReduce (processamento distribuido)

baixa latência, consistência, alta disponibilidade

spark core: Mappers e Reducers que susbstituem o MapReduce
GraphX: processamento de grafos
Spark SQL: 
MLib: machine learning

componentes spark: driver program, cluster manager, workers

A01-P06
análise de dados: como processar e extrair informações com valor

entendimento do negócio, compreender os dados, preparação dos dados, modelagem dos dados, avaliação do modelo, utilização do modelo

visualização exploratória, visualização explanatória

A07.mp4
cloudera.com
HDP (Hortonworks Data Platform)
image virtual box
change root password

Ambari

user maria_dev, admin

views: Files, Hive, Pig, Tez, Workflow

Files: sistema de arquivo
Hive: importar dados, query
Pig: escrever scripts
Tez: grafos, visualização
Workflow: monitorar fluxos

A02-P01
Sumário

A02-P02
ecossistema hadoop: módulos

hadoop é uma plataforma de software open source para o armazenamento e processamento distribuído de conjunto de dados muito grandes em cluster de computadores.

Esquema de módulos
HDFS (sistema de arquivos), YARN (gerenciador de recursos do cluster), MapReduce (distribui o processamento)

tem um nó que gerencia os outros

A02-P03
HDFS
Os dados são distribuidos pelo cluster
tolerância a partição

blocos de 128 MiB são replicados

data node, name node (nome dos arquivos e quais data nodes tem réplica)
um client node, um name node, vários data node
client node solicita leitura ao name node, o client node lê os dados nos data node
client node solicita escrita ao name node, o client node escreve os dados em um data node, este replica em outros data node e retorna ao cliente node, que repassa ao name node.

A02-P04
MapReduce

entrada de dados > mapper > conjunto de chave-valor > agrupa valores com a mesma chave

cliente > YARN > Node Manager (aplicação mestre distribui para aplicações MapReducer)

A02-P05
Alternativas ao MapReducer
MapReduce é dificil fazer SQL

Pig é muito semelhante ao SQL, o script Pig Latin, ainda usa o MapReduce
relação > tabela
tupla > registro (linha)
campo > coluna

tem join

Tez, substitui totalmente o MapReduce
usa grafos acíclicos e lê menos o HDFS

Em vez de usar o MapReduce, pode usar o Tez ou Spark.
Tem CLI chamada Grunt

A02-P06
Gerenciamento de cluster
YARN

primeira versão do hadoop tinha apenas os módulos MapReduce e HDFS. Depois acrescentaram o YARN antes do MapReduce.

é um gerenciador de recursos no nó mestre. Ele distribui trabalho para os demais nós.

ZooKeeper
garante a alta disponibilidade caso tenha uma partição

vários servidores ZooKeeper e vários Clientes ZooKeepers (em nós trabalhadores)

Oozie
também usa grafos acíclicos

A08
HDFS
https://grouplens.org/datasets/movielens/100k/
MovieLens dataset

importou arquivo

hadoop fs -ls
hadoop fs -copyFromLocal u.data /user/maria_dev/

do HDFS para o computador
hadoop fs -copyToLocal u.data

hadoop fs -rm u.data

hadoop fs
mostra lista de comandos

Oozie

A03-P01
Banco de dados
Sumário

A03-P02
Hive é um framework do Hadoop para fazer consultas no HDFS

muito usado para fazer BI (aplicações que fazem consulta SQL)

HiveQL (implementa SQL parcialmente)

Opera em MapReduce ou Tez

schema on read

Arquitetura tem dois módulos: Driver e Master
o Driver compila e executa e repassa ao Master
o Master gerencia as tarefas repassando para os nós trabalhadores

A03-P03
Integração a banco de dados relacionais: Sqoop

sqoop import --connect jdbc:mysql://localhost/foobar --driver com.mysql.jdbc.Driver --table foo
--hive import
--incremental

sqoop export --connect jdbc:mysql://localhost/foobar --driver com.mysql.jdbc.Driver --table exported_foo --export-dir /app/hive/warehouse/foo --input-fields-teminated-by '\001'

A03-P04
NoSQL
CAP: tolerância a Partição é fixa, portanto escolhe ou Consistência ou Disponibilidade

chave-valor, documentos, colunas, grafos

SGBD: HBase

A03-P05
NoSQL externo

Cassandra (chave-valor)
Linguagem CQL
Suporta ACID parcialmente (sem join, chave estrangeira, integridade referencial) (consistência eventual)
Não tem JOIN
Não tem chave estrangeira
Nós distribuidos em forma de anel, sem mestre.

MongoDB (documentos)
JSON
Consistência e Partição, logo pode ficar indisponível, pois tem que retornar dados atuais.

Coleções e documentos
dados embutidos
dado normalizados (faz referência a outro documento)

Servidor primário replica para segundários.

A03-P06
Motores de consulta SQL
Drill: dados em formato JSON, usa SQL, usa ZooKeeper
Phoenix: SQL para HBase, ACID, MapReduce, Spark, Hive, Flume
Presto: SQL para OLAB, projetado pelo Facebook, suporta esquemas relacional e nosql

A09
importa dados e faz consulta

Hive
upload table
u.data
colocou nos nomes das colunas

upload table
u.item
indicou que o separador é |
colocou nos nomes das colunas
colocou o nome da tabela "names"

Query
importou query


HBase
configurou
Pig
conectou pelo shell
hbase shell
criou tabela
consultou
apagou


MySQL
Sqoop
configura uma tabela no MySQL
sqoop import
mostrou os comandos, mas não mostro a prática


Cassandra
Instalou


MongoDB
Instalou

A04-P01
Sumário

Arquitetura Lambda
Ferramentas: Kafka, Flume, Storm, Flink

A04-P02
Processamento de fluxo de dados distribuídos

baixa latência: o processamento é mais rápido que velocidade de entrada de dados

Arquitetura Lambda
- Batch layer: armazena e pré-processa
- Serving layer: disponibiliza os dados da camada anterior
- Speed layer: processa cada dado. Realtime View

A04-P03
Kafka

uma fila de mensagens, um buffer. Baixa latência.
publicação e inscrição, como o dbus no linux

produtor: cria tópico e publica eventos nele
consumidores: se increvem em tópicos

também é usado para processamento de dados
um evento publicado pode ter tempo de duração, ou seja, quanto tempo o Kafka vai guardá-lo esperando clientes

A04-P04
Flume
Processamento em tempo real

geradores de dados > source > channel > sink (destino) > armazenamento

unidade de dado: evento

11:39 comando flume-ng

A04-P05
Storm
sistema de computação distribuída em tempo real

arquitetura mestre-trabalhador

Nimbus gerencia os trabalhadores

Spout: entrada de dados
Bolt: processamento e entrega

um bolt pode entregar dados para outro bolt

cada bolt pode executar várias tarefas ao mesmo tempo, que podem ser agrupadas
All grouping: os dados são replicados para todas as tarefas
None grouping ou Shuffle grouping: dados são distribuidos aleatóriamente para as tarefas
Fields grouping: um campo no dado diz qual tarefa vai processá-lo
Direct grouping: a origem do dado define qual tarefa vai processá-lo

Zookeeper gerencia os Nimbus e os processos trabalhadores. Se um parar de funcionar ele ativa outro.

A04-P06
Flink
processamento de fluxos distribuídos

fluxo ilimitados ou limitados.
ilimitado não tem fim determinado

comunicação em protocolo REST

Fluxos, estados e tempo

fluxo é uma entrada de dado
estado informações armazenada entre eventos
tempo instante em que um evento é criado

APIs
Process functions: controla estados e tempos
DataStream: processamento usando MapReduce e ..Data 
SQL table: consulta sql

A10
Ferramentas: Kafka, Flume, Storm, Flink

Kafka
Publish/subscribe
comandos
Exemplo: separa palavras e conta

Flume
source/channel/sync
comandos
Exemplo de log

Storm
executa sobre o YARN
stream/Spouts/Bolts
comandos
exemplo de contagem de palavras

Flink
Se conecta a vários sistemas de arquivos e banco de dados: HDFS, Cassandra, Kafka, Redis, Elasticsearch ...
comandos
exemplo de contagem de palavras

A05-P01
Spark
Sumário dos módulos

A05-P02
Spark Core
Computação em cluster

driver program > gerenciador de cluster (spark, YARN, ...) -<- Nó trabalhador (executor, cache, [tarefa, ...])

RDD (resilient distributed dataset)
armazena os dados referente à distribuição
mantém cópias em outros nós para resistência à partição

transformações RDD
  map
  flatmap
  filter
  ...

Ações RDD
  reduce
  collect
  take
  ...

A05-P03
Spark SQL

suporte ao hive
structured stream

fonte de dados:
- parquet
- JSON
- ORC
- CSV
- LibVSM
- JDBC

A05-P04
Spark Streaming

Kafka, flume, Kinesis, Sockets TCP
HDFS
Map, reduce

divide em lotes os dados rebidos em stream (DStream), e os processa
Usa RDD para gerenciar

exemplo que separa palavras
comandos

A05-P05
MLLib (machine learning)

aprendizado supervisionado: no treinamento já inclui as respostas corretas
aprendizado não supervisionado: não tem as respostas corretas no treinamento
aprendizado por reforço: recebe reforço para acertos e recebe punição para resposta errada.

pipeline: preparação dos dados > transformadores (formato correto) > Estimadores (qual modelo adequado) > avaliador (testa o modelo)

A05-P06
GraphX (grafos)

extensão de RDD

algoritmos conhecidos:
pageRank: criado pelo google que atribui relevancia a um nó
Triangle: identifica os vértices com maiores relações entre si. Identifica grupos
Connected components: componentes pertencentes a uma rede de computadores

usa grafo de propriedades

tem tabela de vértices e tabela de arestas

vértices
id, propriedade
3,(Rafael, estudante)
7,(João, Pós-doutorando)
5,(Fernando, Professor)
2,(Ingrid, Professor)

arestas
srcId, dstId, propriedade
3,7,colaborador
5,3,orientador
2,5,colega
5,7,pesquisador

A11
SPARK

contagem de avaliação de filmes
comandos

soma de comandas
comandos

encontrar o filme mais popular
explica o esquema
comandos

Spark SQL
comandos sql

A06-P01
sumário

A06-P02
Outros módulo hadoop

Impala
Voltado para BI

Accumulo
BigTable (chave-valor)
segurança

Redis
In-memorian storage

Ignite
cache+storage
ACID + SQL

NiFi
grafos dirigidos

Ambari
Interface web
gerencia os módulos hadoop
Interface principal do pacote hadoop

divide os módulos em:
- core hadoop (hdfs, mapreduce, yarn)
- essential hadoop (hive, pig, tez)
- hadoop support

A06-P03
Data Lake (lago)
Uma estratégia de gerenciamento de dados

Outras estratégias
Data Warehouse
Data Mart (loja de garrafa de água)

Data Swamp (pântano)
dados degradados, não são mais úteis

Níveis de maturidade (Puddle (poça, atende a um pequeno grupo), Pond (lagoa), Lake (lago), Ocean )

A06-P04
Sistemas de recomendação

calda longa
itens versus populariadade

Tipos de recomendação
- Editorial ou curadoria (um humano faz uma lista dos produtos mais vendidos, ou os melhores)
- Agregação simple (usa um critério para fazer automaticamente um top 10)
- Recomendador individualizado (tenta prever o que o usuário precisa)

content-based recommendation system (conta a interação do usuário com o conteúdo)
collaborative filtering (compara usuários)

matriz de consumidores versus itens, contendo as notas dos consumidores para avaliação dos itens

probalidade de um usuário avaliar positivamente um item de uma categoria onde o usuário já avaliou positivamente outro item.
precisa de uma lista das características dos itens.

mostra cálculo de comparação de usuários

A06-P05
Cloud computing

Amazon Elastic MapReduce
IaaS + PaaS
Spark, Hive, HBase, Flink, Presto, etc

Google Cloud Dataproc
IaaS + PaaS
Hive, HBase, Presto, Zeppelin, Zookeeper, Pig, etc

Armazenamento
BigQuery, Cloud Storage, Cloud BigTable

Microsoft Azure
IaaS + PaaS

A06-P06
Design arquitetura Big Data
como combinar componentes

conhecer os componentes e as necessidades do usuário.
o que achamos que precisa, o que o usuário diz que precisa, o que ele realmente precisa

avaliar a experiência do usuário

Working Backwards
Divulga um produto antes de desenvolvê-lo.
A partir das críticas, decide o que fazer, escreve os requisitos.

Requisitos de armazenamento
segurança

A12
Spark streaming
structured streaming
MLlib

Spark streaming
exemplo de análise de log
combinou spark e flume para entregar uma estatística atualizada dos logs.

Spark streaming
análise usando dataframes

MLlib
recomendador de filmes
algoritmo
execução
